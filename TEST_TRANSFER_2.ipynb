{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7616f7f7769f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_transfer_model2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_batch_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_loss_batch_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_episodic_performance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ANU/comp_6470/adapted_deep_embeddings/models/weight_transfer_model2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;31m#create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import struct\n",
    "from array import array\n",
    "import random\n",
    "import numpy as np\n",
    "from models.weight_transfer_model2 import *\n",
    "from utils import classification_batch_evaluation, hist_loss_batch_eval, proto_episodic_performance, proto_performance\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Boilerplate for loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.test_img_fname = 't10k-images-idx3-ubyte'\n",
    "        self.test_lbl_fname = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "        self.train_img_fname = 'train-images-idx3-ubyte'\n",
    "        self.train_lbl_fname = 'train-labels-idx1-ubyte'\n",
    "\n",
    "        self.test_images = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def load_testing(self):\n",
    "        ims, labels = self.load(os.path.join(self.path, self.test_img_fname),\n",
    "                                os.path.join(self.path, self.test_lbl_fname))\n",
    "\n",
    "        self.test_images = self.process_images(ims)\n",
    "        self.test_labels = self.process_labels(labels)\n",
    "\n",
    "        return self.test_images, self.test_labels\n",
    "\n",
    "    def load_training(self):\n",
    "        ims, labels = self.load(os.path.join(self.path, self.train_img_fname),\n",
    "                                os.path.join(self.path, self.train_lbl_fname))\n",
    "\n",
    "        self.train_images = self.process_images(ims)\n",
    "        self.train_labels = self.process_labels(labels)\n",
    "\n",
    "        return self.train_images, self.train_labels\n",
    "\n",
    "    def process_images(self, images):\n",
    "        images_np = np.array(images) / 255.0\n",
    "        return images_np\n",
    "\n",
    "    def process_labels(self, labels):\n",
    "        return np.array(labels)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path_img, path_lbl):\n",
    "        with open(path_lbl, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049,'\n",
    "                                 'got {}'.format(magic))\n",
    "\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(path_img, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051,'\n",
    "                                 'got {}'.format(magic))\n",
    "\n",
    "            image_data = array(\"B\", file.read())\n",
    "\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "\n",
    "        for i in range(size):\n",
    "            images[i][:] = image_data[i * rows * cols:(i + 1) * rows * cols]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def kntl_data_form(self, t1_train, t1_valid, k, n, t2_test):\n",
    "        self.load_testing()\n",
    "        self.load_training()\n",
    "\n",
    "        self.x_train_lt5_full = self.train_images[self.train_labels < 5]\n",
    "        self.y_train_lt5_full = self.train_labels[self.train_labels < 5]\n",
    "        shuffle = np.random.permutation(len(self.y_train_lt5_full))\n",
    "        self.x_train_lt5_full, self.y_train_lt5_full = self.x_train_lt5_full[shuffle], self.y_train_lt5_full[shuffle]\n",
    "\n",
    "        x_test_lt5 = self.test_images[self.test_labels < 5]\n",
    "        y_test_lt5 = self.test_labels[self.test_labels < 5]\n",
    "        shuffle = np.random.permutation(len(y_test_lt5))\n",
    "        self.x_test_lt5_full, self.y_test_lt5_full = x_test_lt5[shuffle], y_test_lt5[shuffle]\n",
    "\n",
    "        self.x_lt5 = np.concatenate((self.x_train_lt5_full, self.x_test_lt5_full), axis=0)\n",
    "        self.y_lt5 = np.concatenate((self.y_train_lt5_full, self.y_test_lt5_full), axis=0)\n",
    "\n",
    "        print('Task 1 full: {0}'.format(len(self.x_lt5)))\n",
    "\n",
    "        shuffle = np.random.permutation(len(self.y_lt5))\n",
    "        self.x_lt5, self.y_lt5 = self.x_lt5[shuffle], self.y_lt5[shuffle]\n",
    "        self.x_valid_lt5, self.y_valid_lt5 = self.x_lt5[:t1_valid], self.y_lt5[:t1_valid]\n",
    "        self.x_train_lt5, self.y_train_lt5 = self.x_lt5[t1_valid:t1_valid + t1_train], self.y_lt5[t1_valid:t1_valid + t1_train]\n",
    "\n",
    "        print('Task 1 training: {0}'.format(len(self.x_train_lt5)))\n",
    "        print('Task 1 validation: {0}'.format(len(self.x_valid_lt5)))\n",
    "\n",
    "        self.x_train_gte5_full = self.train_images[self.train_labels >= 5]\n",
    "        self.y_train_gte5_full = self.train_labels[self.train_labels >= 5] - 5\n",
    "        self.x_test_gte5_full = self.test_images[self.test_labels >= 5]\n",
    "        self.y_test_gte5_full = self.test_labels[self.test_labels >= 5] - 5\n",
    "\n",
    "        self.x_gte5 = np.concatenate((self.x_train_gte5_full, self.x_test_gte5_full), axis=0)\n",
    "        self.y_gte5 = np.concatenate((self.y_train_gte5_full, self.y_test_gte5_full), axis=0)\n",
    "\n",
    "        print('Task 2 full: {0}'.format(len(self.x_gte5)))\n",
    "\n",
    "        self.x_train_gte5 = []\n",
    "        self.y_train_gte5 = []\n",
    "        classes = np.unique(self.y_gte5)\n",
    "        chosen_classes = np.random.choice(classes, n, replace=False)\n",
    "        for c in chosen_classes:\n",
    "            idx = np.random.choice(np.where(self.y_gte5 == c)[0], k, replace=False)\n",
    "            self.x_train_gte5.extend(self.x_gte5[idx])\n",
    "            self.y_train_gte5.extend(self.y_gte5[idx])\n",
    "            self.x_gte5 = np.delete(self.x_gte5, idx, axis=0)\n",
    "            self.y_gte5 = np.delete(self.y_gte5, idx, axis=0)\n",
    "\n",
    "        self.x_train_gte5 = np.array(self.x_train_gte5)\n",
    "        self.y_train_gte5 = np.array(self.y_train_gte5)\n",
    "\n",
    "        assert t2_test <= len(self.y_gte5)\n",
    "\n",
    "        shuffle = np.random.permutation(len(self.y_gte5))\n",
    "        self.x_gte5, self.y_gte5 = self.x_gte5[shuffle], self.y_gte5[shuffle]\n",
    "        self.x_test_gte5, self.y_test_gte5 = self.x_gte5[:t2_test], self.y_gte5[:t2_test]\n",
    "\n",
    "        print('k = {0}, n = {1}'.format(k, n))\n",
    "        print('Task 2 training: {0}'.format(len(self.x_train_gte5)))\n",
    "        print('Task 2 test: {0}\\n'.format(len(self.x_test_gte5)))\n",
    "\n",
    "        return (self.x_train_lt5, self.y_train_lt5), (self.x_valid_lt5, self.y_valid_lt5), (self.x_train_gte5, self.y_train_gte5), (self.x_test_gte5, self.y_test_gte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'dataset': 'mnist', 'data_path': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/datasets/mnist', 't1_train': 8000, 't1_valid': 3000, 'k': 10, 'n': 5, 't2_test': 10000, 'epochs': 500, 'batch_size': 2048, 'learning_rate': 0.005, 'patience': 20, 'percentage_es': 0.01, 'random_seed': 1234, 'replications': 1, 'gpu': '0', 'controller': '/cpu:0', 'save_dir': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer', 'log_file': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer/log.txt', 'command': 'weight_transfer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8543c96cc77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkntl_data_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1_valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MNIST' is not defined"
     ]
    }
   ],
   "source": [
    "data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'],params['k'], params['n'], params['t2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classification(sess, model, data, params, weight_transfer=True):\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_train2, y_train2), (x_test2, y_test2) = data\n",
    "\n",
    "    if weight_transfer:\n",
    "        initial_best_epoch = {'epoch': -1, 'valid_acc': -1}\n",
    "\n",
    "        for epoch in range(1, params['epochs'] + 1):\n",
    "#         for epoch in range(1, 10):\n",
    "\n",
    "            shuffle = np.random.permutation(len(y_train))\n",
    "            x_train, y_train = x_train[shuffle], y_train[shuffle]\n",
    "            for i in range(0, len(y_train), params['batch_size']):\n",
    "                x_train_mb, y_train_mb = x_train[i:i + params['batch_size']], y_train[i:i + params['batch_size']]\n",
    "                sess.run(model.optimize, feed_dict={model.input: x_train_mb, model.target: y_train_mb, model.is_task1: True, model.is_train: True, model.learning_rate: params['learning_rate']})\n",
    "\n",
    "            valid_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], True, x_valid, y=y_valid, stream=True)\n",
    "\n",
    "            print('valid [{} / {}] valid accuracy: {}'.format(epoch, params['epochs'] + 1, valid_acc))\n",
    "            logging.info('valid [{} / {}] valid accuracy: {}'.format(epoch, params['epochs'] + 1, valid_acc))\n",
    "\n",
    "            if valid_acc > initial_best_epoch['valid_acc']:\n",
    "                initial_best_epoch['epoch'] = epoch\n",
    "                initial_best_epoch['valid_acc'] = valid_acc\n",
    "                model.save_model(sess, epoch) \n",
    "                ##Saves the model at the following location : \n",
    "                ##trained_models/mnist/mnist_10_5/weight_transfer/replication1\n",
    "                \n",
    "                ##A\n",
    "                ##Trying to save model as an HDF5 file\n",
    "#                 model.save('my_model.h5')\n",
    "\n",
    "\n",
    "            if epoch - initial_best_epoch['epoch'] >= params['patience']:\n",
    "                print('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                logging.info('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                break\n",
    "\n",
    "        print('Initial training done \\n')\n",
    "        logging.info('Initial training done \\n')\n",
    "\n",
    "        model.restore_model(sess) ##Restores the model after creating it .\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "\n",
    "    transfer_best_epoch = {'epoch': -1, 'train_acc': -1, 'test_acc': -1}\n",
    "    es_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, params['epochs'] + 1):\n",
    "        shuffle = np.random.permutation(len(y_train2))\n",
    "        x_train2, y_train2 = x_train2[shuffle], y_train2[shuffle]\n",
    "        for i in range(0, len(y_train2), params['batch_size']):\n",
    "            x_train_mb, y_train_mb = x_train2[i:i + params['batch_size']], y_train2[i:i + params['batch_size']]\n",
    "            sess.run(model.optimize, feed_dict={model.input: x_train_mb, model.target: y_train_mb, model.is_task1: False, model.is_train: True, model.learning_rate: params['learning_rate']})\n",
    "\n",
    "        train_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], False, x_train2, y=y_train2, stream=True)\n",
    "\n",
    "        print('train [{} / {}] train accuracy: {}'.format(epoch, params['epochs'] + 1, train_acc))\n",
    "        logging.info('train [{} / {}] train accuracy: {}'.format(epoch, params['epochs'] + 1, train_acc))\n",
    "\n",
    "        if train_acc > transfer_best_epoch['train_acc']:\n",
    "            transfer_best_epoch['epoch'] = epoch\n",
    "            transfer_best_epoch['train_acc'] = train_acc\n",
    "            test_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], False, x_test2, y=y_test2, stream=True)\n",
    "            transfer_best_epoch['test_acc'] = test_acc\n",
    "\n",
    "        if epoch % params['patience'] == 0:\n",
    "            acc_diff = transfer_best_epoch['train_acc'] - es_acc\n",
    "            if acc_diff < params['percentage_es'] * es_acc:\n",
    "                print('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                logging.info('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                break\n",
    "            es_acc = transfer_best_epoch['train_acc']\n",
    "\n",
    "    print('Transfer training done \\n')\n",
    "    print('test accuracy: {}'.format(transfer_best_epoch['test_acc']))\n",
    "    logging.info('Transfer training done \\n')\n",
    "    logging.info('test accuracy: {}'.format(transfer_best_epoch['test_acc']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "    model, data = None, None\n",
    "    if params['command'] == 'hist_loss':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTHistModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletHistModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotHistModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetHistModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    elif params['command'] == 'proto':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTProtoModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletProtoModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotProtoModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetProtoModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    elif params['command'] == 'weight_transfer':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTWeightTransferModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'],params['k'], params['n'], params['t2_test'])\n",
    "\t    \n",
    "\t   \n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletWeightTransferModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotWeightTransferModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetWeightTransferModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "\t    \n",
    "    elif params['command'] == 'baseline':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTBaselineModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletBaselineModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotBaselineModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetBaselineModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    else:\n",
    "        print('Unknown model type')\n",
    "        logging.debug('Unknown model type')\n",
    "        quit()\n",
    "    #pdb.set_trace()\n",
    "    return model, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-842214a97557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##initialization_seq = 28883\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replications'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(params['random_seed'])\n",
    "initialization_seq = random.sample(range(50000), params['replications']) \n",
    "##initialization_seq = 28883\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "for rep in range(params['replications']):\n",
    "    tf.reset_default_graph()  #This does siemthing like clearing the default graph stack and i have no idea what does that mean \n",
    "    with tf.Session(config=config) as sess:\n",
    "            #tf.set_random_seed(initialization_seq[rep])\n",
    "        np.random.seed(initialization_seq[rep])\n",
    "            \n",
    "        model, data = get_model(params)\n",
    "            #model : weight_transfer_model.MNISTWeightTransferModel\n",
    "         \n",
    "        assert not model is None\n",
    "        assert not data is None\n",
    "            \n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        params['init'] = init\n",
    "        model.create_saver()\n",
    "            #self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "        sess.run(init)\n",
    "            \n",
    "        rep_path = os.path.join(params['save_dir'], 'replication{}'.format(rep + 1))\n",
    "        os.mkdir(rep_path)\n",
    "        model.config['save_dir_by_rep'] = rep_path\n",
    "\n",
    "        logging.debug('running training/testing')\n",
    "        #pdb.set_trace()\n",
    "        if params['command'] == 'baseline':\n",
    "            train_classification(sess, model, data, params, weight_transfer=False)\n",
    "        elif params['command'] == 'weight_transfer':\n",
    "            train_classification(sess, model, data, params, weight_transfer=True)\n",
    "        elif params['command'] == 'hist_loss':\n",
    "            train_histogram_loss(sess, model, data, params)\n",
    "        elif params['command'] == 'proto':\n",
    "            train_proto_nets(sess, model, data, params)\n",
    "        else:\n",
    "            print('Unknown model type')\n",
    "            logging.debug('Unknown model type')\n",
    "            quit()\n",
    "        #pdb.set_trace()\n",
    "        print(\"Inside run(params): in run_model.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating the t training and testing data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (x_train, y_train), (x_valid, y_valid), (x_train2, y_train2), (x_test2, y_test2) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def hello():\n",
    "    return \"Hello world\"\n",
    "def bye():\n",
    "    return \"Good bye\"\n",
    "print(tf.cond(tf.equal(2,2),hello,bye).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as Sess:\n",
    "    print(tf.cond(tf.equal(2,2),hello,bye).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
