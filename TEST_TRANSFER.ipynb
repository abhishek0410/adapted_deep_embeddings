{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import struct\n",
    "from array import array\n",
    "import random\n",
    "import numpy as np\n",
    "from models.weight_transfer_model import *\n",
    "from utils import classification_batch_evaluation, hist_loss_batch_eval, proto_episodic_performance, proto_performance\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################1.)Boilerplate for loading Data#########################################################\n",
    "############################################################################################################\n",
    "class MNIST():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.test_img_fname = 't10k-images-idx3-ubyte'\n",
    "        self.test_lbl_fname = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "        self.train_img_fname = 'train-images-idx3-ubyte'\n",
    "        self.train_lbl_fname = 'train-labels-idx1-ubyte'\n",
    "\n",
    "        self.test_images = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def load_testing(self):\n",
    "        ims, labels = self.load(os.path.join(self.path, self.test_img_fname),\n",
    "                                os.path.join(self.path, self.test_lbl_fname))\n",
    "\n",
    "        self.test_images = self.process_images(ims)\n",
    "        self.test_labels = self.process_labels(labels)\n",
    "\n",
    "        return self.test_images, self.test_labels\n",
    "\n",
    "    def load_training(self):\n",
    "        ims, labels = self.load(os.path.join(self.path, self.train_img_fname),\n",
    "                                os.path.join(self.path, self.train_lbl_fname))\n",
    "\n",
    "        self.train_images = self.process_images(ims)\n",
    "        self.train_labels = self.process_labels(labels)\n",
    "\n",
    "        return self.train_images, self.train_labels\n",
    "\n",
    "    def process_images(self, images):\n",
    "        images_np = np.array(images) / 255.0\n",
    "        return images_np\n",
    "\n",
    "    def process_labels(self, labels):\n",
    "        return np.array(labels)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path_img, path_lbl):\n",
    "        with open(path_lbl, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049,'\n",
    "                                 'got {}'.format(magic))\n",
    "\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(path_img, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051,'\n",
    "                                 'got {}'.format(magic))\n",
    "\n",
    "            image_data = array(\"B\", file.read())\n",
    "\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "\n",
    "        for i in range(size):\n",
    "            images[i][:] = image_data[i * rows * cols:(i + 1) * rows * cols]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def kntl_data_form(self, t1_train, t1_valid, k, n, t2_test):\n",
    "        self.load_testing()\n",
    "        self.load_training()\n",
    "\n",
    "        self.x_train_lt5_full = self.train_images[self.train_labels < 5]\n",
    "        self.y_train_lt5_full = self.train_labels[self.train_labels < 5]\n",
    "        shuffle = np.random.permutation(len(self.y_train_lt5_full))\n",
    "        self.x_train_lt5_full, self.y_train_lt5_full = self.x_train_lt5_full[shuffle], self.y_train_lt5_full[shuffle]\n",
    "\n",
    "        x_test_lt5 = self.test_images[self.test_labels < 5]\n",
    "        y_test_lt5 = self.test_labels[self.test_labels < 5]\n",
    "        shuffle = np.random.permutation(len(y_test_lt5))\n",
    "        self.x_test_lt5_full, self.y_test_lt5_full = x_test_lt5[shuffle], y_test_lt5[shuffle]\n",
    "\n",
    "        self.x_lt5 = np.concatenate((self.x_train_lt5_full, self.x_test_lt5_full), axis=0)\n",
    "        self.y_lt5 = np.concatenate((self.y_train_lt5_full, self.y_test_lt5_full), axis=0)\n",
    "\n",
    "        print('Task 1 full: {0}'.format(len(self.x_lt5)))\n",
    "\n",
    "        shuffle = np.random.permutation(len(self.y_lt5))\n",
    "        self.x_lt5, self.y_lt5 = self.x_lt5[shuffle], self.y_lt5[shuffle]\n",
    "        self.x_valid_lt5, self.y_valid_lt5 = self.x_lt5[:t1_valid], self.y_lt5[:t1_valid]\n",
    "        self.x_train_lt5, self.y_train_lt5 = self.x_lt5[t1_valid:t1_valid + t1_train], self.y_lt5[t1_valid:t1_valid + t1_train]\n",
    "\n",
    "        print('Task 1 training: {0}'.format(len(self.x_train_lt5)))\n",
    "        print('Task 1 validation: {0}'.format(len(self.x_valid_lt5)))\n",
    "\n",
    "        self.x_train_gte5_full = self.train_images[self.train_labels >= 5]\n",
    "        self.y_train_gte5_full = self.train_labels[self.train_labels >= 5] - 5\n",
    "        self.x_test_gte5_full = self.test_images[self.test_labels >= 5]\n",
    "        self.y_test_gte5_full = self.test_labels[self.test_labels >= 5] - 5\n",
    "\n",
    "        self.x_gte5 = np.concatenate((self.x_train_gte5_full, self.x_test_gte5_full), axis=0)\n",
    "        self.y_gte5 = np.concatenate((self.y_train_gte5_full, self.y_test_gte5_full), axis=0)\n",
    "\n",
    "        print('Task 2 full: {0}'.format(len(self.x_gte5)))\n",
    "\n",
    "        self.x_train_gte5 = []\n",
    "        self.y_train_gte5 = []\n",
    "        classes = np.unique(self.y_gte5)\n",
    "        chosen_classes = np.random.choice(classes, n, replace=False)\n",
    "        for c in chosen_classes:\n",
    "            idx = np.random.choice(np.where(self.y_gte5 == c)[0], k, replace=False)\n",
    "            self.x_train_gte5.extend(self.x_gte5[idx])\n",
    "            self.y_train_gte5.extend(self.y_gte5[idx])\n",
    "            self.x_gte5 = np.delete(self.x_gte5, idx, axis=0)\n",
    "            self.y_gte5 = np.delete(self.y_gte5, idx, axis=0)\n",
    "\n",
    "        self.x_train_gte5 = np.array(self.x_train_gte5)\n",
    "        self.y_train_gte5 = np.array(self.y_train_gte5)\n",
    "\n",
    "        assert t2_test <= len(self.y_gte5)\n",
    "\n",
    "        shuffle = np.random.permutation(len(self.y_gte5))\n",
    "        self.x_gte5, self.y_gte5 = self.x_gte5[shuffle], self.y_gte5[shuffle]\n",
    "        self.x_test_gte5, self.y_test_gte5 = self.x_gte5[:t2_test], self.y_gte5[:t2_test]\n",
    "\n",
    "        print('k = {0}, n = {1}'.format(k, n))\n",
    "        print('Task 2 training: {0}'.format(len(self.x_train_gte5)))\n",
    "        print('Task 2 test: {0}\\n'.format(len(self.x_test_gte5)))\n",
    "\n",
    "        return (self.x_train_lt5, self.y_train_lt5), (self.x_valid_lt5, self.y_valid_lt5), (self.x_train_gte5, self.y_train_gte5), (self.x_test_gte5, self.y_test_gte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 2.)Boilerplate for the WeighTtranfer model #########################################################\n",
    "####################################################################################################################\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import functools\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import abc\n",
    "\n",
    "\n",
    "ABC = abc.ABCMeta('ABC', (object,), {})\n",
    "\n",
    "class Model(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_config(self, config):\n",
    "        c = AttrDict()\n",
    "        for k, v in config.items():\n",
    "            c[k] = v\n",
    "        return c\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def prediction(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def optimize(self):\n",
    "        pass\n",
    "  \n",
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "def doublewrap(function):\n",
    "    '''\n",
    "    A decorator around a decorator allowing use of the original decorator\n",
    "    without parentheses if no arguments are provided. All arguments\n",
    "    must be optional.\n",
    "    '''\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    '''\n",
    "    A decorator for functions that define Tensorflow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result, so that operations are only added to the graph once.\n",
    "    '''\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return decorator\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "PS_OPS = [\n",
    "    'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\n",
    "    'MutableHashTableOfTensors', 'MutableDenseHashTable'\n",
    "]\n",
    "\n",
    "def assign_to_device(device, ps_device):\n",
    "    '''\n",
    "    Returns a function to place variables on the ps_device.\\\n",
    "    If ps_device is not set, then the variables will be placed\n",
    "    on the default device.\n",
    "    '''\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op in PS_OPS:\n",
    "            return ps_device\n",
    "        else:\n",
    "            return device\n",
    "    return _assign\n",
    "\n",
    "def _stride(s):\n",
    "    return [1, s, s, 1]\n",
    "\n",
    "def _relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def _tanh(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "def _softmax(x):\n",
    "    return tf.nn.softmax(x)\n",
    "\n",
    "def _max_pooling(name, x, kernel_size, stride, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=_stride(kernel_size), strides=_stride(stride), padding=padding, name=name)\n",
    "\n",
    "def _conv(name, x, filter_size, in_size, out_size, stride, padding='SAME', bias=True, reuse=None, weight_decay=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        weights = tf.get_variable(\n",
    "            'conv_weights', [filter_size, filter_size, in_size, out_size],\n",
    "            tf.float32, initializer=tf.initializers.random_normal()\n",
    "        )\n",
    "\n",
    "        res = tf.nn.conv2d(x, weights, _stride(stride), padding=padding)\n",
    "\n",
    "        if bias:\n",
    "            biases = tf.get_variable(\n",
    "                'conv_biases', [out_size], tf.float32,\n",
    "                initializer=tf.initializers.zeros()\n",
    "            )\n",
    "            res += biases\n",
    "\n",
    "        if weight_decay:\n",
    "            wd = tf.nn.l2_loss(weights) * weight_decay\n",
    "            tf.add_to_collection('weight_decay', wd)\n",
    "\n",
    "    return res\n",
    "\n",
    "def _fully_connected(name, x, out_size, reuse=None, weight_decay=None):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        weights = tf.get_variable(\n",
    "            'fc_weights', [x.get_shape()[1], out_size], tf.float32,\n",
    "            initializer=tf.initializers.random_normal()\n",
    "        )\n",
    "        biases = tf.get_variable(\n",
    "            'fc_biases', [out_size], tf.float32, initializer=tf.initializers.zeros()\n",
    "        )\n",
    "\n",
    "        if weight_decay:\n",
    "            wd = tf.nn.l2_loss(weights) * weight_decay\n",
    "            tf.add_to_collection('weight_decay', wd)\n",
    "\n",
    "        return tf.nn.xw_plus_b(x, weights, biases)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class WeightTransferModel(Model):\n",
    "\n",
    "    def __init__(self, config,freeze):\n",
    "        print(\"Inside the WeightTransferModel constructor function \",config)\n",
    "        self.config = self.get_config(config)\n",
    "        self.saver = None\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.freeze =freeze\n",
    "\n",
    "    def create_saver(self):\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "    def save_model(self, sess, step):\n",
    "        # self.saver.save(sess, os.path.join(self.config.save_dir_by_rep, 'model.ckpt'), global_step=step)\n",
    "        self.saver.save(sess, os.path.join(self.config.save_dir_by_rep, 'model.h5'), global_step=step)\n",
    "\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        checkpoint = tf.train.latest_checkpoint(self.config.save_dir_by_rep)\n",
    "        if checkpoint is None:\n",
    "            sys.exit('Cannot restore model that does not exist')\n",
    "        self.saver.restore(sess, checkpoint)\n",
    "\n",
    "    def get_single_device(self):\n",
    "        devices = get_available_gpus()\n",
    "        d = self.config.controller\n",
    "        if devices:\n",
    "            d = devices[0]\n",
    "        return d\n",
    "\n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        ##Note that the scope of tf.GraphKeys is prediction\n",
    "#         pass\n",
    "        d = self.get_single_device()\n",
    "        with tf.device(assign_to_device(d, self.config.controller)):\n",
    "           \n",
    "            pred = self.prediction\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, \n",
    "                labels=tf.one_hot(self.target, self.config.n)))\n",
    "#             if self.freeze:\n",
    "#                 #OLD APPROACH\n",
    "#                 optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,var_list=[self.x] )\n",
    "\n",
    "    \n",
    "#             else:\n",
    "#                 optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "                  \n",
    "         \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            \n",
    "            ##Below command will freeze both the weight and biases for the convolution layers, they might or might not\n",
    "            ## be used based on the below if condition :\n",
    "            freeze_conv1  =tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"prediction/conv1\")\n",
    "            freeze_conv2  =tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"prediction/conv2\")\n",
    "            #pdb.set_trace()\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                if self.freeze:\n",
    "                    train_op = optimizer.minimize(cost,var_list=freeze_conv1+ freeze_conv2)\n",
    "                else:\n",
    "                    train_op = optimizer.minimize(cost)\n",
    "    \n",
    "            return train_op, cost\n",
    "\n",
    "    @define_scope(scope='stream_metrics')\n",
    "    def metrics(self):\n",
    "        d = self.get_single_device()\n",
    "        with tf.device(assign_to_device(d, self.config.controller)):\n",
    "            pred = self.prediction\n",
    "            acc, update_acc = tf.metrics.accuracy(self.target, tf.argmax(_softmax(pred), axis=1))\n",
    "            return update_acc\n",
    "\n",
    "\n",
    "class MNISTWeightTransferModel(WeightTransferModel):\n",
    "\n",
    "    def __init__(self, config,freeze):\n",
    "        super().__init__(config,freeze)\n",
    "        self.input = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.target = tf.placeholder(tf.int32, [None])\n",
    "        self.is_task1 = tf.placeholder(tf.bool)\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.metrics\n",
    "        self.freeze=freeze\n",
    "        \n",
    "        \n",
    "\n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        d = self.get_single_device()\n",
    "        with tf.device(assign_to_device(d, self.config.controller)):\n",
    "            x = self.input\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            x = _relu(_conv('conv1', x, 3, x.get_shape()[-1], 32, 1))\n",
    "            x = _max_pooling('pool2', _relu(_conv('conv2', x, 3, x.get_shape()[-1], 32, 1)), 2, 2)\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "            self.x = _relu(_fully_connected('fc1', x, 128))\n",
    "            x1 = lambda: _fully_connected('fc3', x, self.config.n)\n",
    "            x2 = lambda: _fully_connected('fc4', x, self.config.n)\n",
    "            x = tf.cond(tf.equal(self.is_task1, tf.constant(True)), x1, x2)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 3.)Other important methods#########################################################\n",
    "####################################################################################################################\n",
    "\n",
    "def train_classification(sess, model, data, params, weight_transfer=True):\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_train2, y_train2), (x_test2, y_test2) = data\n",
    "    ##x_train = y_train = 8000 ;x_valid = y_valid=3000;x_train2 =y_train2 = 50 ; x_test2 =y_test2 =10000\n",
    "    pdb.set_trace()\n",
    " \n",
    "    temp_learning_rate_source_training = params['learning_rate']\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    if weight_transfer:\n",
    "        initial_best_epoch = {'epoch': -1, 'valid_acc': -1}\n",
    "        \n",
    "\n",
    "  \n",
    "    \n",
    "    with open(\"/home/abhishek/Desktop/{}_{}_{}.txt\".format(params[\"dataset\"],params[\"k\"],params[\"n\"]), \"a\") as f:\n",
    "        for epoch in range(1, params['epochs'] + 1):\n",
    "            shuffle = np.random.permutation(len(y_train))\n",
    "            x_train, y_train = x_train[shuffle], y_train[shuffle]\n",
    "            \n",
    "\n",
    "            for i in range(0, len(y_train), params['batch_size']):\n",
    "                x_train_mb, y_train_mb = x_train[i:i + params['batch_size']], y_train[i:i + params['batch_size']]\n",
    "                \n",
    "#                 sess.run(model.optimize, feed_dict={model.input: x_train_mb, model.target: y_train_mb, model.is_task1: True, model.is_train: True, model.learning_rate: params['learning_rate']})\n",
    "                sess.run(model.optimize, feed_dict={model.input: x_train_mb, model.target: y_train_mb, model.is_task1: True, model.is_train: True, model.learning_rate: temp_learning_rate_source_training})\n",
    "      \n",
    "                \n",
    "\n",
    "            valid_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], True, x_valid, y=y_valid, stream=True)\n",
    "\n",
    "            print('valid [{} / {}] valid accuracy: {} learning Rate :{}'.format(epoch, params['epochs'] + 1, valid_acc,temp_learning_rate_source_training),file =f)\n",
    "            logging.info('valid [{} / {}] valid accuracy: {}'.format(epoch, params['epochs'] + 1, valid_acc))\n",
    "\n",
    "            if valid_acc > initial_best_epoch['valid_acc']:\n",
    "                initial_best_epoch['epoch'] = epoch\n",
    "                initial_best_epoch['valid_acc'] = valid_acc\n",
    "                model.save_model(sess, epoch) \n",
    "                ##Saves the model at the following location : \n",
    "                ##trained_models/mnist/mnist_10_5/weight_transfer/replication1\n",
    "                \n",
    "                ##A\n",
    "                ##Trying to save model as an HDF5 file\n",
    "#                 model.save('my_model.h5')\n",
    "\n",
    "\n",
    "            if epoch - initial_best_epoch['epoch'] >= params['patience']:\n",
    "                print('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                logging.info('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                break\n",
    "\n",
    "        print('Initial training done \\n')\n",
    "        logging.info('Initial training done \\n')\n",
    "\n",
    "        model.restore_model(sess) ##Restores the model after creating it .\n",
    "\n",
    "    transfer_best_epoch = {'epoch': -1, 'train_acc': -1, 'test_acc': -1}\n",
    "    es_acc = 0.0\n",
    "#     model.freeze = True\n",
    "#     print(\"Model parameters are \",model.freeze)\n",
    "\n",
    "\n",
    "    with open(\"/home/abhishek/Desktop/{}_{}_{}.txt\".format(params[\"dataset\"],params[\"k\"],params[\"n\"]), \"a\") as f:\n",
    "        temp_learning_rate_target_training = 0.01\n",
    "        for epoch in range(1, params['epochs'] + 1):\n",
    "            shuffle = np.random.permutation(len(y_train2))\n",
    "            x_train2, y_train2 = x_train2[shuffle], y_train2[shuffle]\n",
    "        \n",
    "        \n",
    "            if epoch%5==0:\n",
    "                temp_learning_rate_target_training = temp_learning_rate_target_training *0.1\n",
    "                \n",
    "            for i in range(0, len(y_train2), params['batch_size']):\n",
    "                x_train_mb, y_train_mb = x_train2[i:i + params['batch_size']], y_train2[i:i + params['batch_size']]\n",
    "                sess.run(model.optimize, feed_dict={model.input: x_train_mb, model.target: y_train_mb, model.is_task1: False, model.is_train: True, model.learning_rate: params['learning_rate']})\n",
    "\n",
    "            train_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], False, x_train2, y=y_train2, stream=True)\n",
    "\n",
    "            print('train [{} / {}] train accuracy: {} learning Rate:{} '.format(epoch, params['epochs'] + 1, train_acc,temp_learning_rate_target_training),file=f)\n",
    "            logging.info('train [{} / {}] train accuracy: {}'.format(epoch, params['epochs'] + 1, train_acc))\n",
    "\n",
    "            if train_acc > transfer_best_epoch['train_acc']:\n",
    "                transfer_best_epoch['epoch'] = epoch\n",
    "                transfer_best_epoch['train_acc'] = train_acc\n",
    "                test_acc = classification_batch_evaluation(sess, model, model.metrics, params['batch_size'], False, x_test2, y=y_test2, stream=True)\n",
    "                transfer_best_epoch['test_acc'] = test_acc\n",
    "\n",
    "            if epoch % params['patience'] == 0:\n",
    "                acc_diff = transfer_best_epoch['train_acc'] - es_acc\n",
    "                if acc_diff < params['percentage_es'] * es_acc:\n",
    "                    print('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                    logging.info('Early Stopping Epoch: {}\\n'.format(epoch))\n",
    "                    break\n",
    "                es_acc = transfer_best_epoch['train_acc']\n",
    "\n",
    "        print('Transfer training done \\n')\n",
    "        print('test accuracy: {}'.format(transfer_best_epoch['test_acc']))\n",
    "        logging.info('Transfer training done \\n')\n",
    "        logging.info('test accuracy: {}'.format(transfer_best_epoch['test_acc']))\n",
    "    \n",
    "def get_model(params):\n",
    "    model, data = None, None\n",
    "    print(\"params:\",params)\n",
    "    if params['command'] == 'hist_loss':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTHistModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletHistModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotHistModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetHistModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    elif params['command'] == 'proto':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTProtoModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletProtoModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotProtoModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetProtoModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    elif params['command'] == 'weight_transfer':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTWeightTransferModel(params,freeze = False)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'],params['k'], params['n'], params['t2_test'])\n",
    "\t    \n",
    "\t   \n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletWeightTransferModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotWeightTransferModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetWeightTransferModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "\t    \n",
    "    elif params['command'] == 'baseline':\n",
    "        if params['dataset'] == 'mnist':\n",
    "            model = MNISTBaselineModel(params)\n",
    "            data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'], \n",
    "                params['k'], params['n'], params['t2_test'])\n",
    "        elif params['dataset'] == 'isolet':\n",
    "            model = IsoletBaselineModel(params)\n",
    "            data = Isolet(params['data_path']).kntl_data_form(250, params['n'], params['k'], params['n'])\n",
    "        elif params['dataset'] == 'omniglot':\n",
    "            model = OmniglotBaselineModel(params)\n",
    "            data = Omniglot(params['data_path']).kntl_data_form(params['n'], params['k'], params['n'])\n",
    "        else:\n",
    "            model = TinyImageNetBaselineModel(params)\n",
    "            data = TinyImageNet(params['data_path']).kntl_data_form(350, params['n'], params['k'], params['n'])\n",
    "    else:\n",
    "        print('Unknown model type')\n",
    "        logging.debug('Unknown model type')\n",
    "        quit()\n",
    "    #pdb.set_trace()\n",
    "    return model, data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 full: 35735\n",
      "Task 1 training: 8000\n",
      "Task 1 validation: 3000\n",
      "Task 2 full: 34265\n",
      "k = 10, n = 5\n",
      "Task 2 training: 50\n",
      "Task 2 test: 10000\n",
      "\n",
      "params: {'dataset': 'mnist', 'data_path': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/datasets/mnist', 't1_train': 8000, 't1_valid': 3000, 'k': 10, 'n': 5, 't2_test': 10000, 'epochs': 500, 'batch_size': 2048, 'learning_rate': 0.005, 'patience': 20, 'percentage_es': 0.01, 'random_seed': 1234, 'replications': 1, 'gpu': '0', 'controller': '/cpu:0', 'save_dir': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer', 'log_file': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer/log.txt', 'command': 'weight_transfer'}\n",
      "Inside the WeightTransferModel constructor function  {'dataset': 'mnist', 'data_path': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/datasets/mnist', 't1_train': 8000, 't1_valid': 3000, 'k': 10, 'n': 5, 't2_test': 10000, 'epochs': 500, 'batch_size': 2048, 'learning_rate': 0.005, 'patience': 20, 'percentage_es': 0.01, 'random_seed': 1234, 'replications': 1, 'gpu': '0', 'controller': '/cpu:0', 'save_dir': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer', 'log_file': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer/log.txt', 'command': 'weight_transfer'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0915 03:11:11.712298 140655394404160 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0915 03:11:11.712993 140655394404160 deprecation.py:323] From /home/abhishek/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 full: 35735\n",
      "Task 1 training: 8000\n",
      "Task 1 validation: 3000\n",
      "Task 2 full: 34265\n",
      "k = 10, n = 5\n",
      "Task 2 training: 50\n",
      "Task 2 test: 10000\n",
      "\n",
      "> <ipython-input-4-9514fe38b625>(9)train_classification()\n",
      "-> temp_learning_rate_source_training = params['learning_rate']\n",
      "(Pdb) pdb.set_trace()\n",
      "(Pdb) cont\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 03:11:36.379333 140655394404160 deprecation_wrapper.py:119] From /home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/utils.py:7: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0915 03:11:36.380294 140655394404160 deprecation_wrapper.py:119] From /home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/utils.py:7: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0915 03:11:36.381140 140655394404160 deprecation_wrapper.py:119] From /home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/utils.py:8: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "W0915 03:11:45.329012 140655394404160 deprecation.py:323] From /home/abhishek/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program interrupted. (Use 'cont' to resume).\n",
      "--Return--\n",
      "> /home/abhishek/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py(1429)_call_tf_sessionrun()->[2.7551804]\n",
      "-> run_metadata)\n"
     ]
    }
   ],
   "source": [
    "####################### 4.)Running the code #########################################################\n",
    "####################################################################################################################\n",
    "params={'dataset': 'mnist', 'data_path': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/datasets/mnist', 't1_train': 8000, 't1_valid': 3000, 'k': 10, 'n': 5, 't2_test': 10000, 'epochs': 500, 'batch_size': 2048, 'learning_rate': 0.005, 'patience': 20, 'percentage_es': 0.01, 'random_seed': 1234, 'replications': 1, 'gpu': '0', 'controller': '/cpu:0', 'save_dir': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer', 'log_file': '/home/abhishek/Desktop/ANU/comp_6470/adapted_deep_embeddings/trained_models/mnist/mnist_10_5/weight_transfer/log.txt', 'command': 'weight_transfer'}\n",
    "data = MNIST(params['data_path']).kntl_data_form(params['t1_train'], params['t1_valid'],params['k'], params['n'], params['t2_test'])\n",
    "\n",
    "random.seed(params['random_seed'])\n",
    "initialization_seq = random.sample(range(50000), params['replications']) \n",
    "##initialization_seq = 28883\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "for rep in range(params['replications']):\n",
    "    tf.reset_default_graph()  #This does siemthing like clearing the default graph stack and i have no idea what does that mean \n",
    "    with tf.Session(config=config) as sess:\n",
    "            #tf.set_random_seed(initialization_seq[rep])\n",
    "        np.random.seed(initialization_seq[rep])\n",
    "            \n",
    "        model, data = get_model(params)\n",
    "            #model : weight_transfer_model.MNISTWeightTransferModel\n",
    "         \n",
    "        assert not model is None\n",
    "        assert not data is None\n",
    "            \n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        params['init'] = init\n",
    "        model.create_saver()\n",
    "            #self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "        sess.run(init)\n",
    "            \n",
    "        rep_path = os.path.join(params['save_dir'], 'replication{}'.format(rep + 1))\n",
    "        os.mkdir(rep_path)\n",
    "        model.config['save_dir_by_rep'] = rep_path\n",
    "\n",
    "        logging.debug('running training/testing')\n",
    "        #pdb.set_trace()\n",
    "        if params['command'] == 'baseline':\n",
    "            train_classification(sess, model, data, params, weight_transfer=False)\n",
    "        elif params['command'] == 'weight_transfer':\n",
    "            train_classification(sess, model, data, params, weight_transfer=True)\n",
    "        elif params['command'] == 'hist_loss':\n",
    "            train_histogram_loss(sess, model, data, params)\n",
    "        elif params['command'] == 'proto':\n",
    "            train_proto_nets(sess, model, data, params)\n",
    "        else:\n",
    "            print('Unknown model type')\n",
    "            logging.debug('Unknown model type')\n",
    "            quit()\n",
    "        #pdb.set_trace()\n",
    "        print(\"Inside run(params): in run_model.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(\"/home/abhishek/Desktop/a.txt\") as f1:\n",
    "                lines = f1.readlines()\n",
    "                lines = [l for l in lines if \"ROW\" in l]\n",
    "                with open(\"/home/abhishek/Desktop/mnist_10_5_q1_q4.txt\", \"w\") as f:\n",
    "                    f.writelines(lines)\n",
    "#                     print(\"Targett Training Begins\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/abhishek/Desktop/a.txt\") as f:\n",
    "    with open(\"/home/abhishek/Desktop/mnist_10_5_q1_q4.txt\", \"w\") as f1:\n",
    "        for x in f.readlines():\n",
    "            f1.write(x)\n",
    "            print(\"Hahah\",file =f1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/abhishek/Desktop/mnist_10_5_q1_q4.txt\", \"w\") as f:\n",
    "        print(\"Hahah\",file =f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "29\n",
      "0\n",
      "1st if\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "decay_after_epoch = 3\n",
    "learning_rate=0.05\n",
    "for epoch in range(1, 100):\n",
    "    print((epoch-decay_after_epoch)%30)\n",
    "    if epoch%decay_after_epoch==0 and epoch <=decay_after_epoch:\n",
    "        learning_rate = learning_rate *0.1\n",
    "        print(\"1st if\")\n",
    "    elif epoch-decay_after_epoch%30==0:\n",
    "        print(\"elif\")\n",
    "        learning_rate = learning_rate *0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000006e-10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0000000000000006e-10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
